import pandas as pdimport numpy as npimport os #os è un libreria per individuare la directory dove ci si trovafrom datetime import datetimefrom sklearn import tree#import weka.core.jvm as jvmfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_scorefrom sklearn.ensemble import RandomForestRegressorfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitfrom sklearn import metricsfrom sklearn import datasetsfrom sklearn.multiclass import OutputCodeClassifierfrom sklearn.svm import *from sklearn.naive_bayes import MultinomialNBimport sklearnfrom sklearn import svmfrom sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifierimport imblearnfrom imblearn.pipeline import Pipelinefrom imblearn.over_sampling import SMOTEfrom imblearn.combine import SMOTEENNfrom sklearn.linear_model import LogisticRegressionfrom sklearn.linear_model import Perceptronfrom collections import Counterfrom sklearn.datasets import make_classificationfrom matplotlib import pyplotfrom numpy import whereimport numpyfrom imblearn.under_sampling import NearMissimport matplotlib.pyplot as plt# Training set uploadtraining = pd.read_csv('/Users/giulia/Unimib/Data Science Lab/progetto fastweb/progettodslab/training.csv', sep=';')    # verifica valori null all'interno del trainingtraining = training.dropna()len(training['KIT_ID'].unique())counter = Counter(training['KIT_ID'])print(counter)#Trasformazione TS in datetimetraining['TS'] = pd.to_datetime(training['TS'])#training['TS'] = pd.to_numeric(training['TS'], downcast='float', errors='ignore')# type(training.loc[0,'TS'] )training.loc[0,'TS']print('0 ' + str(len(training[training['VAR_CLASS'] == 0])))#16521526print('1 ' + str(len(training[training['VAR_CLASS'] == 1])))#36print('2 ' + str(len(training[training['VAR_CLASS'] == 2])))#472str(len(training[training['VAR_CLASS'] == 2]))#Analisi TStraining['TS'] = pd.to_datetime(training['TS'])training['TS'].dt.year.unique()#2018training['TS'].dt.month.unique()#11training['TS'].dt.day.unique()#1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30training['VAR_CLASS'].unique()#1,2,3training[training['NUM_CLI'] >= 100]# training['USAGE'].sort_values(ascending=True)trainingOrderByUSAGE = training.sort_values(by=['USAGE'])trainingOrderByAVG = training.sort_values(by=['AVG_SPEED_DW'])training.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))training['KIT_ID'].describe()descriptiveQuantity = training[['USAGE','AVG_SPEED_DW','NUM_CLI']].describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))descriptiveQuantitydef prepareTraining():    training = pd.read_csv('/Users/giulia/Unimib/Data Science Lab/progetto fastweb/progettodslab/training.csv', sep=';')        #da inserire il TS    X = training[['USAGE','KIT_ID','AVG_SPEED_DW','NUM_CLI']]    y = training['VAR_CLASS']        X = X.to_numpy()    y = y.to_numpy()    return (X,y)X, y = prepareTraining()#In terms of machine learning, Clf is an estimator instance, which is used to store model.#We use clf to store trained model values, which are further used to predict value, based on the previously stored weights.# Generate a synthetic imbalanced classification dataset#Synthetic Minority Over-sampling Techniqueoversample = SMOTE(random_state=100,k_neighbors=2)X, y = oversample.fit_resample(X, y)counter = Counter(y)print(counter)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)## definisco il datasetX, y = make_classification() # NON RUNNO ORA## definisco il modellomodelLRovr = LogisticRegression(multi_class='ovr')## fitto il modellomodelLRovr.fit(X_train, y_train)## eseguo predizioney_pred_lr = modelLRovr.predict(X_test)#### PROVA CONFUSION MATRIX FOR MULTICLASS ####confusion = confusion_matrix(y_test, y_pred_lr)print('Confusion Matrix\n')print(confusion)#accuracy_score, precision_score, recall_score, f1_scoreprint('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred_lr)))print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred_lr, average='micro')))print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred_lr, average='micro')))print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred_lr, average='micro')))print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred_lr, average='macro')))print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred_lr, average='macro')))print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred_lr, average='macro')))print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred_lr, average='weighted')))print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred_lr, average='weighted')))print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred_lr, average='weighted')))print('\nClassification Report\n')print(classification_report(y_test, y_pred_lr, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_lr))print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_lr))print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_lr)))#precision:reali positivi, maggiore è, minore errore di falsi positivi#recall:percentuale valori predetti correttamente, quindi se elevata sono stati predetti pochi positivi#nella classe di negativi### ALTERNATIVA SU COME SCRIVERE IL CODICE PER OVR#########################OneVsRestClassifier#############################################al posto di clf possiamo mettere qualsiasi altra robaclf = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)y_pred = clf.predict(X_test)accuracy_score(y_test, y_pred) #0.43#### PROVA 3 da sistemare ??pipe = Pipeline([('sampl', SMOTEENN()),                 ('clf', MultinomialNB())])ovr = OneVsRestClassifier(pipe)ovr.fit(X, y)######### PERCEPTRON #########X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)np.array(y_test)np.array(y_train)# creo il modello previsionale con Perceptron e definisco i parametri#imposto un max di iterazioni pari a 40, tol indica l'indice di tolleranza#sufficiente per l'uscita dal ciclo, eta indica tasso di apprendimento del#modello.ppn = Perceptron(max_iter=40, tol=0.001, eta0=0.01, random_state=0)# fitto il modelloppn.fit(X_train, y_train) # eseguo la predizioney_pred_ppn = ppn.predict(X_test)#### PROVA CONFUSION MATRIX FOR MULTICLASS ####confusion_ppn = confusion_matrix(y_test, y_pred_ppn)print('Confusion Matrix\n')print(confusion_ppn)#accuracy_score, precision_score, recall_score, f1_scoreprint('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred_ppn)))print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred_ppn, average='micro')))print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred_ppn, average='micro')))print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred_ppn, average='micro')))print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred_ppn, average='macro')))print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred_ppn, average='macro')))print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred_ppn, average='macro')))print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred_ppn, average='weighted')))print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred_ppn, average='weighted')))print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred_ppn, average='weighted')))print('\nClassification Report\n')print(classification_report(y_test, y_pred_ppn, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_ppn))print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_ppn))print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_ppn)))