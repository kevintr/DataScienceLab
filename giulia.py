import pandas as pdimport numpy as npimport os #os è un libreria per individuare la directory dove ci si trovafrom datetime import datetimefrom sklearn import treeimport weka.core.jvm as jvmfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score,recall_scorefrom sklearn.ensemble import RandomForestRegressorfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitfrom sklearn import metricsfrom sklearn import datasetsfrom sklearn.multiclass import OutputCodeClassifierfrom sklearn.svm import *from sklearn.naive_bayes import MultinomialNBimport sklearnfrom sklearn import svmfrom sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifierimport imblearnfrom imblearn.pipeline import Pipelinefrom imblearn.over_sampling import SMOTEfrom imblearn.combine import SMOTEENNfrom sklearn.linear_model import LogisticRegressionfrom sklearn.linear_model import Perceptronfrom collections import Counterfrom sklearn.datasets import make_classificationfrom matplotlib import pyplotfrom numpy import whereimport numpyfrom imblearn.under_sampling import NearMissimport matplotlib.pyplot as plt# Training set uploadtraining = pd.read_csv('/Users/giulia/Unimib/Data Science Lab/progetto fastweb/progettodslab/training.csv', sep=';')    # verifica valori null all'interno del trainingtraining = training.dropna()len(training['KIT_ID'].unique())counter = Counter(training['KIT_ID'])print(counter)#Trasformazione TS in datetimetraining['TS'] = pd.to_datetime(training['TS'])#training['TS'] = pd.to_numeric(training['TS'], downcast='float', errors='ignore')# type(training.loc[0,'TS'] )training.loc[0,'TS']print('0 ' + str(len(training[training['VAR_CLASS'] == 0])))#16521526print('1 ' + str(len(training[training['VAR_CLASS'] == 1])))#36print('2 ' + str(len(training[training['VAR_CLASS'] == 2])))#472str(len(training[training['VAR_CLASS'] == 2]))#Analisi TStraining['TS'] = pd.to_datetime(training['TS'])training['TS'].dt.year.unique()#2018training['TS'].dt.month.unique()#11training['TS'].dt.day.unique()#1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30training['VAR_CLASS'].unique()#1,2,3training[training['NUM_CLI'] >= 100]# training['USAGE'].sort_values(ascending=True)trainingOrderByUSAGE = training.sort_values(by=['USAGE'])trainingOrderByAVG = training.sort_values(by=['AVG_SPEED_DW'])training.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))training['KIT_ID'].describe()descriptiveQuantity = training[['USAGE','AVG_SPEED_DW','NUM_CLI']].describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))descriptiveQuantitydef prepareTraining():    training = pd.read_csv('/Users/giulia/Unimib/Data Science Lab/progetto fastweb/progettodslab/training.csv', sep=';')        #da inserire il TS    X = training[['USAGE','KIT_ID','AVG_SPEED_DW','NUM_CLI']]    y = training['VAR_CLASS']        X = X.to_numpy()    y = y.to_numpy()    return (X,y)X,y = prepareTraining()#In terms of machine learning, Clf is an estimator instance, which is used to store model.#We use clf to store trained model values, which are further used to predict value, based on the previously stored weights.# Generate a synthetic imbalanced classification dataset#Synthetic Minority Over-sampling Techniqueoversample = SMOTE(random_state=100,k_neighbors=2)X, y = oversample.fit_resample(X, y)counter = Counter(y)print(counter)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)## definisco il datasetX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=None)## definisco il modellomodelLRovr = LogisticRegression(multi_class='ovr')## fitto il modellomodelLRovr.fit(X_train, y_train)## eseguo predizioney_pred = modelLRovr.predict(X_test)accuracy_score(y_test, y_pred)#0.43recall_score(y_test, y_pred)y_true = yconfusion_matrix(y_test, y_pred, labels=[0, 1, 2])y_trueprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))print(confusion_matrix(y_test,y_pred))print(classification_report(y_test,y_pred)) #precision:reali positivi, maggiore è, minore errore di falsi positivi#recall:percentuale valori predetti correttamente, quindi se elevata sono stati predetti pochi positivi#nella classe di negativi####### RIPRENDI DA QUI ################### ALTERNATIVA SU COME SCRIVERE IL CODICE PER OVR#########################OneVsRestClassifier#############################################al posto di clf possiamo mettere qualsiasi altra robaclf = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)y_pred = clf.predict(X_test)accuracy_score(y_test, y_pred) #0.43#### PROVA 3 da sistemare ??pipe = Pipeline([('sampl', SMOTEENN()),                 ('clf', MultinomialNB())])ovr = OneVsRestClassifier(pipe)ovr.fit(X, y)