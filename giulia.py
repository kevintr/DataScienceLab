import pandas as pdimport numpy as npimport os #os è un libreria per individuare la directory dove ci si trovafrom datetime import datetime, timefrom sklearn import tree#import weka.core.jvm as jvmfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_scorefrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressorfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, ShuffleSplitfrom sklearn import metricsfrom sklearn import datasetsfrom sklearn.multiclass import OutputCodeClassifierfrom sklearn.svm import *from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNBimport sklearnfrom sklearn import svmfrom sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifierimport imblearnfrom imblearn.pipeline import Pipelinefrom imblearn.over_sampling import SMOTEfrom imblearn.combine import SMOTEENNfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptronfrom sklearn.neural_network import MLPClassifier, MLPRegressorfrom collections import Counterfrom sklearn.datasets import make_classificationfrom matplotlib import pyplotfrom numpy import whereimport numpyfrom imblearn.under_sampling import NearMissimport matplotlib.pyplot as pltimport warningsimport sklearn.exceptionswarnings.filterwarnings("ignore", category=sklearn.exceptions.UndefinedMetricWarning)# Training set uploadtraining = pd.read_csv('/Users/giulia/Unimib/Data Science Lab/progetto fastweb/progettodslab/training.csv', sep=';')    # verifica valori null all'interno del trainingtraining = training.dropna()#len(training['KIT_ID'].unique())#counter = Counter(training['KIT_ID'])#print(counter)#Trasformazione TS in datetimetraining['TS'] = pd.to_datetime(training['TS'])#training['TS'] = pd.to_numeric(training['TS'], downcast='float', errors='ignore')# type(training.loc[0,'TS'] )training.loc[0,'TS']#print('0 ' + str(len(training[training['VAR_CLASS'] == 0])))#16521526#print('1 ' + str(len(training[training['VAR_CLASS'] == 1])))#36#print('2 ' + str(len(training[training['VAR_CLASS'] == 2])))#472#str(len(training[training['VAR_CLASS'] == 2]))#Analisi TStraining['TS'] = pd.to_datetime(training['TS'])training['TS'].dt.year.unique()#2018training['TS'].dt.month.unique()#11training['TS'].dt.day.unique()#1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30training['VAR_CLASS'].unique()#1,2,3training[training['NUM_CLI'] >= 100]# training['USAGE'].sort_values(ascending=True)trainingOrderByUSAGE = training.sort_values(by=['USAGE'])trainingOrderByAVG = training.sort_values(by=['AVG_SPEED_DW']) training.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))training['KIT_ID'].describe()descriptiveQuantity = training[['USAGE','AVG_SPEED_DW','NUM_CLI']].describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))descriptiveQuantity# CREO DIVERSI DATAFRAME PER KIT_ID# kit_id1 3409364152 # kit_id2 1629361016# kit_id3 2487219358 training_diff_kit = training[((training['KIT_ID']!=3409364152) & (training['KIT_ID']!=1629361016) & (training['KIT_ID']!=2487219358))]training_kit_id1 = training[training.KIT_ID==3409364152]training_kit_id2 = training[training.KIT_ID==1629361016]training_kit_id3 = training[training.KIT_ID==2487219358]training_kit_anomalie = training[((training['KIT_ID']==3409364152) | (training['KIT_ID']==1629361016) | (training['KIT_ID']==2487219358))]training['KIT_ID'].unique()# 3460333709, 3755290149, 2615826724#3826188756, 2818312083,       #2152306337training2 = training[((training['KIT_ID']==3409364152) |                      (training['KIT_ID']==1629361016) |                      (training['KIT_ID']==2487219358) |                      (training['KIT_ID']==3460333709) |                      (training['KIT_ID']==3755290149) |                      (training['KIT_ID']==2615826724) |                      (training['KIT_ID']==3826188756) |                      (training['KIT_ID']==2818312083) |                      (training['KIT_ID']==2152306337))]########## Finally EVALUATE how well it does on the training and test data.#########print("Test score " + str(reg.score(X_test, Y_test)))#print("Train score " + str(reg.score(X_test, Y_test)))################ SOLE PROVE SUL TRAINING KIT ANOMALIE SOLO VAR_CLASS 0 ####################PROVA = training_kit_anomalie.drop(training_kit_anomalie[training_kit_anomalie.VAR_CLASS >0].index)print('0 ' + str(len(PROVA[PROVA['VAR_CLASS'] == 0]))) #19927print('1 ' + str(len(PROVA[PROVA['VAR_CLASS'] == 1]))) #0print('2 ' + str(len(PROVA[PROVA['VAR_CLASS'] == 2]))) #0def prepareTraining(training_kit_anomalie):       epoch = datetime.utcfromtimestamp(0)    training_kit_anomalie.loc[:,'TS'] = pd.to_datetime(training_kit_anomalie['TS'])    training_kit_anomalie.loc[:,'TS'] = training_kit_anomalie.loc[:,'TS'] - epoch    training_kit_anomalie.loc[:,'TS'] = training_kit_anomalie.loc[:,'TS'].dt.total_seconds()        X = training_kit_anomalie.loc[:,['TS','KIT_ID','USAGE','NUM_CLI']]    y = training_kit_anomalie.loc[:,'VAR_CLASS']        X = X.to_numpy()    y = y.to_numpy()    return (X,y)X, y = prepareTraining(training)z = training['NUM_CLI']w = training['VAR_CLASS']np.corrcoef(z, w)# baseline model and test harness for the glass identification dataset#from numpy import mean#from numpy import std#from pandas import read_csv#from sklearn.preprocessing import LabelEncoder#from sklearn.model_selection import cross_val_score#from sklearn.model_selection import RepeatedStratifiedKFoldX.shapey.shapecounter = Counter(y)print(counter)oversample = SMOTE(random_state=100,k_neighbors=5)X, y = oversample.fit_resample(X, y)counter = Counter(y)print(counter)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)############ DECISION TREE #############  X, y = prepareTraining(training)oversample = SMOTE(random_state=100,k_neighbors=5)X, y = oversample.fit_resample(X, y)counter = Counter(y)print(counter)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)clf1 = tree.DecisionTreeClassifier()clf1.fit(X_train, y_train)y_pred_DT = clf1.predict(X_test)confusionDT = confusion_matrix(y_test, y_pred_DT)print(confusionDT)#[[5952    0    0]# [   0 5950    0]# [   0    0 6033]]#intero dataset e oversample#[[4954811       1       0]# [      0 4957736       0]# [      0       0 4956826]]print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred_DT))) #1.0000#intero dataset e oversample acc 1.0000print('\nClassification Report\n')print(classification_report(y_test, y_pred_DT, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      1.00      1.00      5952# Var_Class 1       1.00      1.00      1.00      5950# Var_Class 2       1.00      1.00      1.00      6033#    accuracy                           1.00     17935#   macro avg       1.00      1.00      1.00     17935#weighted avg       1.00      1.00      1.00     17935#intero dataset e oversample#              precision    recall  f1-score   support# Var_Class 0       1.00      1.00      1.00   4954812# Var_Class 1       1.00      1.00      1.00   4957736# Var_Class 2       1.00      1.00      1.00   4956826#    accuracy                           1.00  14869374#   macro avg       1.00      1.00      1.00  14869374#weighted avg       1.00      1.00      1.00  14869374print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_DT)) #0.00print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_DT)) #0.00print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_DT))) #0.00### per verificare se il modello è un buon modello ###X, y = prepareTraining(training_diff_kit)y_pred_DT_DIFF = clf1.predict(X)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y, y_pred_DT_DIFF))) #0.9734#dopo intero dataset e oversample acc 1.0000confusionDT_DIFF = confusion_matrix(y, y_pred_DT_DIFF)print(confusionDT_DIFF)#[[16062131    25572   413896]# [       0        0        0]# [       0        0        0]]# dopo intero dataset e oversample#[[16501599]]print('\nClassification Report\n')print(classification_report(y, y_pred_DT_DIFF, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      0.97      0.99  16501599# Var_Class 1       0.00      0.00      0.00         0# Var_Class 2       0.00      0.00      0.00         0#    accuracy                           0.97  16501599#   macro avg       0.33      0.32      0.33  16501599#weighted avg       1.00      0.97      0.99  16501599########### RANDOM FOREST ########### X, y = prepareTraining(training)y.shapeoversample = SMOTE(random_state=100,k_neighbors=5)X, y = oversample.fit_resample(X, y)counter = Counter(y)print(counter)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)clf2 = RandomForestClassifier()clf2.fit(X_train, y_train) y_pred_RF = clf2.predict(X_test)confusionRF = confusion_matrix(y_test, y_pred_RF)print(confusionRF) #over e split vari 0#[[5950    1    1]# [   0 5950    0]# [   0    0 6033]]# inserendo l'intero dataset training e impostando l'oversample #[[4954811       1       0]# [      0 4957736       0]# [      0       0 4956826]]print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred_RF))) #0.9999#anche sull'intero dataset training e con oversample abbiamo accuracy 1.0000print('\nClassification Report\n')print(classification_report(y_test, y_pred_RF, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      1.00      1.00      5952# Var_Class 1       1.00      1.00      1.00      5950# Var_Class 2       1.00      1.00      1.00      6033#    accuracy                           1.00     17935#   macro avg       1.00      1.00      1.00     17935#weighted avg       1.00      1.00      1.00     17935#con intero dataset training e oversample otteniamo tale classification report#              precision    recall  f1-score   support# Var_Class 0       1.00      1.00      1.00   4954812# Var_Class 1       1.00      1.00      1.00   4957736# Var_Class 2       1.00      1.00      1.00   4956826#    accuracy                           1.00  14869374#   macro avg       1.00      1.00      1.00  14869374#weighted avg       1.00      1.00      1.00  14869374print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_RF)) #over e split 0.00016print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_RF)) #over e split 0.00027print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_RF))) #over e split 0.01669### per verificare se il modello è un buon modello ###X, y = prepareTraining(training_diff_kit)y_pred_RF_DIFF = clf2.predict(X)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y, y_pred_RF_DIFF))) #0.9928#dopo intero dataset acc 1.0000confusionRF_DIFF = confusion_matrix(y, y_pred_RF_DIFF)print(confusionRF_DIFF)#[[16383330     2569   115700]# [       0        0        0]# [       0        0        0]]print('\nClassification Report\n')print(classification_report(y, y_pred_RF_DIFF, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      0.99      1.00  16501599# Var_Class 1       0.00      0.00      0.00         0# Var_Class 2       0.00      0.00      0.00         0#    accuracy                           0.99  16501599#   macro avg       0.33      0.33      0.33  16501599#weighted avg       1.00      0.99      1.00  16501599##################### PROVA FIT E PRED SU TEST.CSV ####################test = pd.read_csv('/Users/giulia/Unimib/Data Science Lab/progetto fastweb/progettodslab/test.csv', sep=';')  test = test.dropna()#Trasformazione TS in datetime#test['TS'] = pd.to_datetime(test['TS'])#test.loc[0,'TS']def prepareTraining1(test):       epoch = datetime.utcfromtimestamp(0)    test.loc[:,'TS'] = pd.to_datetime(test['TS'])    test.loc[:,'TS'] = test.loc[:,'TS'] - epoch    test.loc[:,'TS'] = test.loc[:,'TS'].dt.total_seconds()       T = test.loc[:,['TS','KIT_ID','USAGE','NUM_CLI']]    T = T.to_numpy()    return TT = prepareTraining1(test)t_pred = clf1.predict(T)test.loc[:,'VAR_CLASS'] = pd.Series(t_pred)len(test[(test['VAR_CLASS']== 1) | (test['VAR_CLASS']== 2)]['KIT_ID'].unique())# RF ne predice 2!!# DT ne predice 3!!len(test['KIT_ID'].unique()) #2121test[test['VAR_CLASS'] == 2]## RF 1970831019; 1824349749## DT 109826360; 1970831019; 1824349749#   RF               TS  USAGE      KIT_ID  AVG_SPEED_DW  NUM_CLI  VAR_CLASS#8182992   1.550153e+09      0  1970831019         79003        9          2#8185099   1.550154e+09      0  1970831019         79003        9          2#12707331  1.550797e+09      0  1824349749         93087       19          2#12709440  1.550798e+09      0  1824349749         93087       19          2#12711553  1.550798e+09      0  1824349749         93087       19          2#12713666  1.550798e+09      0  1824349749         93087       19          2# DT                 TS  USAGE      KIT_ID  AVG_SPEED_DW  NUM_CLI  VAR_CLASS#2131241   1.549286e+09      0   109826360         62689       12          2#2135435   1.549286e+09      0   109826360         62689       12          2#2141725   1.549287e+09      0   109826360         62689       12          2#8182992   1.550153e+09      0  1970831019         79003        9          2#8185099   1.550154e+09      0  1970831019         79003        9          2#12707331  1.550797e+09      0  1824349749         93087       19          2#12709440  1.550798e+09      0  1824349749         93087       19          2#12711553  1.550798e+09      0  1824349749         93087       19          2#12713666  1.550798e+09      0  1824349749         93087       19          2training[training['VAR_CLASS'] == 2]#                    TS     USAGE      KIT_ID  AVG_SPEED_DW  NUM_CLI  VAR_CLASS#3239768   1.541538e+09  24436920  3409364152         85320       26          2#3241672   1.541538e+09  22772337  3409364152         85320       26          2#3243608   1.541538e+09  23335770  3409364152         85320       26          2#3245658   1.541539e+09  37466972  3409364152         85320       26          2#3247498   1.541539e+09  28558597  3409364152         85320       26          2#               ...       ...         ...           ...      ...        ...#16512149  1.543621e+09        97  2487219358         75411        3          2#16514126  1.543621e+09        99  2487219358         75411        3          2#16516103  1.543622e+09        98  2487219358         75411        3          2#16518080  1.543622e+09        99  2487219358         75411        3          2#16520057  1.543622e+09        98  2487219358         75411        3          2test[test['VAR_CLASS'] != 0]test.loc[(test['KIT_ID'] == 109826360) & (test['TS'] == '1.549286e+09')]provakit1_test = test.loc[test['KIT_ID'] == 1970831019]provakit2_test = test.loc[test['KIT_ID'] == 1824349749]provakit3_test = test.loc[test['KIT_ID'] == 109826360]training.loc[(training['KIT_ID'] == 3409364152) & (training['USAGE'] == 0)] #nessun risultatotraining.loc[(training['KIT_ID'] == 1629361016) & (training['USAGE'] == 0)] #nessun risultatotraining.loc[(training['KIT_ID'] == 2487219358) & (training['USAGE'] == 0)] #nessun risultatoprint('0 ' + str(len(test[test['VAR_CLASS'] == 0]))) # RF 16926840 # DT 16926837print('1 ' + str(len(test[test['VAR_CLASS'] == 1]))) # RF 0 # DT 0print('2 ' + str(len(test[test['VAR_CLASS'] == 2]))) # RF 6 # DT 9########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersnetwork = tf.keras.models.Sequential()network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))network.add(layers.Dense(10, activation='softmax'))############ LOGISTIC REGRESSION ###########mod_PROVA = LogisticRegression()# define the ovr strategyovr = OneVsRestClassifier(mod_PROVA)# fit modelovr.fit(X_train, y_train)# make predictionsy_pred_prova = ovr.predict(X_test)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred_prova))) #0.5160#intero dataset e oversample 0.5359confusion_mod_PROVA = confusion_matrix(y_test, y_pred_prova)print(confusion_mod_PROVA)#[[2265 1669 2018]#[ 385 3635 1930]#[ 992 1686 3355]]#intero dataset#[[2562618 1167981 1224213]# [ 769271 3235253  953212]# [1105486 1680838 2170502]]print('\nClassification Report\n')print(classification_report(y_test, y_pred_prova, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       0.62      0.38      0.47      5952# Var_Class 1       0.52      0.61      0.56      5950# Var_Class 2       0.46      0.56      0.50      6033#    accuracy                           0.52     17935#   macro avg       0.53      0.52      0.51     17935#weighted avg       0.53      0.52      0.51     17935#intero dataset#              precision    recall  f1-score   support# Var_Class 0       0.58      0.52      0.55   4954812# Var_Class 1       0.53      0.65      0.59   4957736# Var_Class 2       0.50      0.44      0.47   4956826#    accuracy                           0.54  14869374#   macro avg       0.54      0.54      0.53  14869374#weighted avg       0.54      0.54      0.53  14869374print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_prova)) #over e split 0.6512print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_prova)) #over e split 0.9781print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_prova))) #over e split 0.9890### per verificare se il modello è un buon modello ###X, y = prepareTraining(training_diff_kit)y_pred_provaDIFF = ovr.predict(X)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y, y_pred_provaDIFF))) #0.5048confusion_mod_PROVA_DIFF = confusion_matrix(y, y_pred_provaDIFF)print(confusion_mod_PROVA_DIFF)#[[8330196 2475613 5695790]#[      0       0       0]#[      0       0       0]]print('\nClassification Report\n')print(classification_report(y, y_pred_provaDIFF, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      0.50      0.67  16501599# Var_Class 1       0.00      0.00      0.00         0# Var_Class 2       0.00      0.00      0.00         0#    accuracy                           0.50  16501599#   macro avg       0.33      0.17      0.22  16501599#weighted avg       1.00      0.50      0.67  16501599######################## PERCEPTRON ######################### ppn = Perceptron(max_iter=21000, tol=0.001, eta0=0.01, random_state=0)ppn.fit(X_train, y_train)y_pred_ppn = ppn.predict(X_test)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred_ppn))) #0.3319confusionPPN = confusion_matrix(y_test, y_pred_ppn)print(confusionPPN)#[[5952    0    0]# [5950    0    0]# [6033    0    0]]print('\nClassification Report\n')print(classification_report(y_test, y_pred_ppn, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       0.33      1.00      0.50      5952# Var_Class 1       0.00      0.00      0.00      5950# Var_Class 2       0.00      0.00      0.00      6033#    accuracy                           0.33     17935#   macro avg       0.11      0.33      0.17     17935#weighted avg       0.11      0.33      0.17     17935X, y = prepareTraining(training_diff_kit)y_predPPN2 = ppn.predict(X)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y, y_predPPN2))) #0.8670confusionPPN2 = confusion_matrix(y, y_predPPN2)print(confusionPPN2)#[[14307360   193289  2000950]# [       0        0        0]# [       0        0        0]]print('\nClassification Report\n')print(classification_report(y, y_predPPN2, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      0.87      0.93  16501599# Var_Class 1       0.00      0.00      0.00         0# Var_Class 2       0.00      0.00      0.00         0#    accuracy                           0.87  16501599#   macro avg       0.33      0.29      0.31  16501599#weighted avg       1.00      0.87      0.93  16501599##### ASSOLUTAMENTE NO ################## SVC ################ per fittare sul modello che ha solo 0, ci impiega#circa 3 ORE infatti si decide di non utilizzare questo metodo. X, y = prepareTraining(training_kit_anomalie)oversample = SMOTE(random_state=100,k_neighbors=5)X, y = oversample.fit_resample(X, y)counter = Counter(y)print(counter)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)clf = svm.SVC(decision_function_shape='ovr')clf.fit(X_train, y_train)y_pred = clf.predict(X_test)confusionSVC = confusion_matrix(y_test, y_pred)print(confusionSVC)#[[1504 2017 2431]# [ 124 3896 1930]# [ 126 1897 4010]]print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred))) #0.84 #n_sample: 10.000 acc 0.8392 #kit_anom 0.8485 #con split 0.8420#over e split 0.5247print('\nClassification Report\n')print(classification_report(y_test, y_pred, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       0.86      0.25      0.39      5952# Var_Class 1       0.50      0.65      0.57      5950# Var_Class 2       0.48      0.66      0.56      6033#    accuracy                           0.52     17935#   macro avg       0.61      0.52      0.50     17935#weighted avg       0.61      0.52      0.50     17935print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))### MACRO F1:### This is macro-averaged F1-score. It calculates metrics for each class individually and### then takes unweighted mean of the measures. As we have seen from figure “Precision,### Recall and F1-score for Each Class”,print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))### WEIGHTED F1:### The last one is weighted-averaged F1-score. Unlike Macro F1, it takes a weighted mean### of the measures. The weights for each class are the total number of samples of that class.print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred)) #over e split 0.6237print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred)) #over e split 0.9048print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #over e split 0.9512### per verificare se il modello è un buon modello ###X, y = prepareTraining(training_diff_kit)y_pred_DIFF = clf.predict(X)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y, y_pred_DIFF))) #0.3023confusion_mod_DIFF = confusion_matrix(y, y_pred_DIFF)print(confusion_mod_DIFF)#[[[4988086 3397453 8116060]# [      0       0       0]# [      0       0       0]]print('\nClassification Report\n')print(classification_report(y, y_pred_DIFF, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      0.30      0.46  16501599# Var_Class 1       0.00      0.00      0.00         0# Var_Class 2       0.00      0.00      0.00         0#    accuracy                           0.30  16501599#   macro avg       0.33      0.10      0.15  16501599#weighted avg       1.00      0.30      0.46  16501599########### COMPLEMENT NB ########## for imbalanced data sets #GRISIAMO??X, y = prepareTraining(training_kit_anomalie)counter1 = Counter(y)print(counter1)oversample = SMOTE(random_state=100,k_neighbors=5)X, y = oversample.fit_resample(X, y)counter = Counter(y)print(counter)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)clf3 = ComplementNB()clf3.fit(X_train, y_train) y_pred_CNB = clf3.predict(X_test)confusionCNB = confusion_matrix(y_test, y_pred_CNB)print(confusionCNB)#[[1765 1758 2429]# [ 158 3862 1930]# [ 284 1739 4010]]print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y_test, y_pred_CNB))) #0.5373print('\nClassification Report\n')print(classification_report(y_test, y_pred_CNB, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#precision    recall  f1-score   support# Var_Class 0       0.80      0.30      0.43      5952# Var_Class 1       0.52      0.65      0.58      5950# Var_Class 2       0.48      0.66      0.56      6033#    accuracy                           0.54     17935#   macro avg       0.60      0.54      0.52     17935#weighted avg       0.60      0.54      0.52     17935print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_CNB)) #over e split 0.6220print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_CNB)) #over e split 0.9186print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_CNB))) #over e split 0.9584### per verificare se il modello è un buon modello ###X, y = prepareTraining(training_diff_kit)y_pred_CNB_DIFF = clf3.predict(X)print('\nAccuracy: {:.4f}\n'.format(accuracy_score(y, y_pred_CNB_DIFF))) #0.3580confusionCNB_DIFF = confusion_matrix(y, y_pred_CNB_DIFF)print(confusionCNB_DIFF)#[[5907375 2371833 8222391]# [      0       0       0]# [      0       0       0]]print('\nClassification Report\n')print(classification_report(y, y_pred_CNB_DIFF, target_names=['Var_Class 0', 'Var_Class 1', 'Var_Class 2']))#              precision    recall  f1-score   support# Var_Class 0       1.00      0.36      0.53  16501599# Var_Class 1       0.00      0.00      0.00         0# Var_Class 2       0.00      0.00      0.00         0#    accuracy                           0.36  16501599#   macro avg       0.33      0.12      0.18  16501599#weighted avg       1.00      0.36      0.53  16501599#############################################################################################################################################################################################################################################################################################################################################